
loading data
ToxicityRegressor(
  (layer_1): Linear(in_features=801, out_features=16, bias=True)
  (layer_2): Linear(in_features=16, out_features=32, bias=True)
  (layer_3): Linear(in_features=32, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=229432, bias=True)
  (relu): ReLU()
)
Begin training
fold 1
epoch 1
train loss: 0.6843900037747318 test loss: 0.6195360164554908
epoch 2
train loss: 0.6843900067298901 test loss: 0.6195360132316817
epoch 3
train loss: 0.6843900153416395 test loss: 0.6195360325745364
epoch 4
train loss: 0.6843900154162647 test loss: 0.6195360277388227
epoch 5
train loss: 0.6843900026404286 test loss: 0.6195360035602544
epoch 6
train loss: 0.6843899996554201 test loss: 0.6195360261269182
epoch 7
train loss: 0.6843900024314781 test loss: 0.6195360153808878
epoch 8
train loss: 0.6843900112969531 test loss: 0.6195360293507273
epoch 9
train loss: 0.6843900092074472 test loss: 0.6195359992618423
epoch 10
train loss: 0.6843900072373416 test loss: 0.6195360186046969
epoch 11
train loss: 0.6843900173863703 test loss: 0.6195360153808878
epoch 12
train loss: 0.6843900097447486 test loss: 0.6195359922769226
epoch 13
train loss: 0.6843900072970418 test loss: 0.6195360304253303
epoch 14
train loss: 0.6843900012673247 test loss: 0.6195360250523151
epoch 15
train loss: 0.6843900041777079 test loss: 0.6195360132316817
epoch 16
train loss: 0.684390009864149 test loss: 0.6195360148435863
epoch 17
train loss: 0.6843900004912226 test loss: 0.6195360325745364
epoch 18
train loss: 0.6843899990584185 test loss: 0.6195360191419984
epoch 19
train loss: 0.6843900156550654 test loss: 0.6195360024856514
epoch 20
train loss: 0.6843900125506566 test loss: 0.619536022903109
epoch 21
train loss: 0.6843899932973522 test loss: 0.6195360277388227
epoch 22
train loss: 0.6843900049388851 test loss: 0.6195360019483499
epoch 23
train loss: 0.684390010580551 test loss: 0.6195360121570787
epoch 24
train loss: 0.6843900142222613 test loss: 0.6195360255896166
epoch 25
train loss: 0.6843900138939104 test loss: 0.6195360298880288
epoch 26
train loss: 0.6843899991181187 test loss: 0.6195360116197772
epoch 27
train loss: 0.6843900191475252 test loss: 0.6195360132316817
epoch 28
train loss: 0.6843900044911339 test loss: 0.6195360325745364
epoch 29
train loss: 0.6843900086402955 test loss: 0.6195360234404106
epoch 30
train loss: 0.6843900050881355 test loss: 0.6195360164554908
epoch 31
train loss: 0.6843900029090794 test loss: 0.6195360282761242
epoch 32
train loss: 0.6843900115656039 test loss: 0.6195360132316817
epoch 33
train loss: 0.6843900108193517 test loss: 0.6195360255896166
epoch 34
train loss: 0.6843900068791405 test loss: 0.6195360175300939
epoch 35
train loss: 0.6843900078641934 test loss: 0.6195360175300939
epoch 36
train loss: 0.6843900075358424 test loss: 0.6195360245150136
epoch 37
train loss: 0.6843900114163534 test loss: 0.6195360272015212
epoch 38
train loss: 0.6843900013270249 test loss: 0.6195360100078726
epoch 39
train loss: 0.6843900032374303 test loss: 0.6195360196792999
epoch 40
train loss: 0.6843900019240265 test loss: 0.6195360110824757
epoch 41
train loss: 0.6843900032374303 test loss: 0.619536020753903
epoch 42
train loss: 0.6843900058343876 test loss: 0.6195360116197772
epoch 43
train loss: 0.684390003461306 test loss: 0.619535991739621
epoch 44
train loss: 0.6843900153565645 test loss: 0.6195360314999333
epoch 45
train loss: 0.6843900046105341 test loss: 0.6195360019483499
epoch 46
train loss: 0.6843900210579307 test loss: 0.6195360239777121
epoch 47
train loss: 0.6843899972972635 test loss: 0.6195360492308833
epoch 48
train loss: 0.6843900111477026 test loss: 0.6195360175300939
epoch 49
train loss: 0.6843900058045376 test loss: 0.6195360180673953
epoch 50
train loss: 0.6843900087298458 test loss: 0.6195360132316817
epoch 51
train loss: 0.6843900036553314 test loss: 0.6195360057094604
epoch 52
train loss: 0.684390009147747 test loss: 0.6195360057094604
epoch 53
train loss: 0.6843900001927217 test loss: 0.6195359912023195
epoch 54
train loss: 0.6843900018046263 test loss: 0.6195360288134257
epoch 55
train loss: 0.6843900016255258 test loss: 0.6195360368729484
epoch 56
train loss: 0.6843900163416173 test loss: 0.619536022903109
epoch 57
train loss: 0.6843900060433382 test loss: 0.6195360272015212
epoch 58
train loss: 0.6843900097447486 test loss: 0.6195360293507273
epoch 59
train loss: 0.6843900097447486 test loss: 0.6195360196792999
epoch 60
train loss: 0.6843900062224387 test loss: 0.6195360143062848
epoch 61
train loss: 0.6843900006106228 test loss: 0.6195360067840635
epoch 62
train loss: 0.6843900116850041 test loss: 0.6195360288134257
epoch 63
train loss: 0.6843900068791405 test loss: 0.6195360175300939
epoch 64
train loss: 0.6843899990285685 test loss: 0.6195360331118379
epoch 65
train loss: 0.6843900103417504 test loss: 0.6195360261269182
epoch 66
train loss: 0.6843900050284354 test loss: 0.619536021828506
epoch 67
train loss: 0.6843900067298901 test loss: 0.6195360159181893
epoch 68
train loss: 0.6843900139536105 test loss: 0.6195360282761242
epoch 69
train loss: 0.6843900065806398 test loss: 0.6195360105451742
epoch 70
train loss: 0.6843900116850041 test loss: 0.619536021828506
epoch 71
train loss: 0.6843900157744657 test loss: 0.6195360282761242
epoch 72
train loss: 0.6843900154162647 test loss: 0.6195360272015212
epoch 73
train loss: 0.6843900078343432 test loss: 0.6195360100078726
epoch 74
train loss: 0.6843900063418391 test loss: 0.6195360234404106
epoch 75
train loss: 0.6843899963420608 test loss: 0.619536005172159
epoch 76
train loss: 0.684390009147747 test loss: 0.619536022903109
epoch 77
train loss: 0.6843900088492462 test loss: 0.6195360239777121
epoch 78
train loss: 0.6843900132073585 test loss: 0.6195360110824757
epoch 79
train loss: 0.6843900029687795 test loss: 0.6195360164554908
epoch 80
train loss: 0.6843900143118116 test loss: 0.6195359981872393
epoch 81
train loss: 0.6843900108193517 test loss: 0.6195360100078726
epoch 82
train loss: 0.6843900016852259 test loss: 0.6195360132316817
epoch 83
train loss: 0.6843900198937773 test loss: 0.6195360186046969
epoch 84
train loss: 0.6843900064015392 test loss: 0.6195360379475515
epoch 85
train loss: 0.6843900069388408 test loss: 0.6195360024856514
epoch 86
train loss: 0.6843900040433826 test loss: 0.6195360212912044
epoch 87
train loss: 0.6843900111775527 test loss: 0.6195360234404106
epoch 88
train loss: 0.6843900065806398 test loss: 0.6195360261269182
epoch 89
train loss: 0.6843900003718222 test loss: 0.6195360078586666
epoch 90
train loss: 0.6843900132670586 test loss: 0.6195360148435863
epoch 91
train loss: 0.6843900066104898 test loss: 0.6195360454697728
epoch 92
train loss: 0.6843900028792292 test loss: 0.6195360137689833
epoch 93
train loss: 0.6843900061627386 test loss: 0.6195360272015212
epoch 94
train loss: 0.6843900140133107 test loss: 0.6195360153808878
epoch 95
train loss: 0.6843900019240265 test loss: 0.6195360067840635
epoch 96
train loss: 0.6843900047000844 test loss: 0.6195360089332695
epoch 97
train loss: 0.6843899997748205 test loss: 0.6195360196792999
epoch 98
train loss: 0.6843900121924555 test loss: 0.6195360406340591
epoch 99
train loss: 0.6843900118641046 test loss: 0.619536020753903
epoch 100
train loss: 0.6843900038792071 test loss: 0.6195360476189788
epoch 101
train loss: 0.6843900110581523 test loss: 0.6195360110824757
epoch 102
train loss: 0.6843900044911339 test loss: 0.6195360250523151
epoch 103
train loss: 0.6843899961331102 test loss: 0.6195360030229529
epoch 104
train loss: 0.6843900109387521 test loss: 0.6195360121570787
epoch 105
train loss: 0.6843900063418391 test loss: 0.6195360110824757
epoch 106
train loss: 0.6843900093268475 test loss: 0.6195360024856514
epoch 107
train loss: 0.6843899969092124 test loss: 0.6195360175300939
epoch 108
train loss: 0.6843900052075359 test loss: 0.6195360331118379
epoch 109
train loss: 0.6843900063418391 test loss: 0.6195360148435863
epoch 110
train loss: 0.6843900111477026 test loss: 0.6195360078586666
epoch 111
train loss: 0.6843900078343432 test loss: 0.6195360406340591
epoch 112
train loss: 0.684390012013355 test loss: 0.6195359976499377
epoch 113
train loss: 0.6843900007598733 test loss: 0.6195360121570787
epoch 114
train loss: 0.6843900111477026 test loss: 0.6195360411713606
epoch 115
train loss: 0.6843900145804623 test loss: 0.619536005172159
epoch 116
train loss: 0.6843900061925886 test loss: 0.6195360261269182
epoch 117
train loss: 0.6843900185206735 test loss: 0.6195360121570787
epoch 118
train loss: 0.6843900115357537 test loss: 0.6195360261269182
epoch 119
train loss: 0.6843900101626499 test loss: 0.6195360186046969
epoch 120
train loss: 0.6843900038045819 test loss: 0.6195360175300939
epoch 121
train loss: 0.6843900065507896 test loss: 0.6195360100078726
epoch 122
train loss: 0.6843900051478357 test loss: 0.6195360282761242
epoch 123
train loss: 0.6843900133267588 test loss: 0.6195360336491393
epoch 124
train loss: 0.6843900037150317 test loss: 0.6195360196792999
epoch 125
train loss: 0.684390007058241 test loss: 0.6195360250523151
epoch 126
train loss: 0.6843900054164864 test loss: 0.6195360132316817
epoch 127
train loss: 0.684390011282028 test loss: 0.6195360175300939
epoch 128
train loss: 0.684390019087825 test loss: 0.6195359858293044
epoch 129
train loss: 0.6843900107895016 test loss: 0.6195360191419984
epoch 130
train loss: 0.6843900120730553 test loss: 0.6195360261269182
epoch 131
train loss: 0.684390015177464 test loss: 0.6195360261269182
epoch 132
train loss: 0.6843900005210726 test loss: 0.6195360089332695
epoch 133
train loss: 0.6843900126103568 test loss: 0.6195359981872393
epoch 134
train loss: 0.6843900069985409 test loss: 0.6195360148435863
epoch 135
train loss: 0.6843900065209396 test loss: 0.6195360180673953
epoch 136
train loss: 0.6843900123118559 test loss: 0.6195360336491393
epoch 137
train loss: 0.6843900104462256 test loss: 0.6195360325745364
epoch 138
train loss: 0.6843900101626499 test loss: 0.6195360057094604
epoch 139
train loss: 0.6843900038941322 test loss: 0.619536036335647
epoch 140
train loss: 0.6843900075358424 test loss: 0.6195359987245408
epoch 141
train loss: 0.684390004909035 test loss: 0.6195360325745364
epoch 142
train loss: 0.6843900098044489 test loss: 0.6195360347237424
epoch 143
train loss: 0.6843900028195291 test loss: 0.619536036335647
epoch 144
train loss: 0.6843900129984078 test loss: 0.6195360105451742
epoch 145
train loss: 0.6843900157147655 test loss: 0.6195360035602544
epoch 146
train loss: 0.6843900083716448 test loss: 0.6195360153808878
epoch 147
train loss: 0.6843900051776858 test loss: 0.6195359852920028
