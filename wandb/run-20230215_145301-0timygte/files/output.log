
loading data
ToxicityRegressor(
  (layer_1): Linear(in_features=801, out_features=16, bias=True)
  (layer_2): Linear(in_features=16, out_features=32, bias=True)
  (layer_3): Linear(in_features=32, out_features=16, bias=True)
  (layer_out): Linear(in_features=16, out_features=229432, bias=True)
  (relu): ReLU()
)
Begin training
fold 1
epoch 1
train loss: 0.7106117269041806 test loss: 118656221583420.6
epoch 2
train loss: 0.7016885856220774 test loss: 40992377534702.33
epoch 3
train loss: 0.7013490538078325 test loss: 3598567731418.1304
epoch 4
train loss: 0.7011457650958773 test loss: 17734294962951.574
epoch 5
train loss: 0.7007785213834853 test loss: 24710861925422.453
epoch 6
train loss: 0.7003558984272461 test loss: 3411794416722.8086
epoch 7
train loss: 0.6989273659871867 test loss: 1201320128536.2366
epoch 8
train loss: 0.695931406296935 test loss: 714391808901120.0
epoch 9
train loss: 0.6939224690229921 test loss: 551360908988133.25
epoch 10
train loss: 0.6881838502313794 test loss: 395950486569056.94
epoch 11
train loss: 0.6768860341354457 test loss: 2868767031270147.5
epoch 12
train loss: 0.6685527362065757 test loss: 1.114410077044135e+16
epoch 13
train loss: 0.660087027229612 test loss: 1.5362171120222144e+16
epoch 14
train loss: 0.6637197318816496 test loss: 2.0731005927314904e+16
epoch 15
train loss: 0.6517993698768937 test loss: 6.381346828031324e+16
epoch 16
train loss: 0.6396488996874521 test loss: 5.131912842430271e+16
epoch 17
train loss: 0.6342057163965708 test loss: 1.722992541573966e+17
epoch 18
train loss: 0.6352050534216636 test loss: 2.3522588913603853e+17
epoch 19
train loss: 0.6262364716722388 test loss: 4.020009882518996e+17
epoch 20
train loss: 0.6266872929868378 test loss: 4.2148019485404986e+17
epoch 21
train loss: 0.6158129977694656 test loss: 7.520095712452554e+17
epoch 22
train loss: 0.6158920683500643 test loss: 7.582444955959446e+17
epoch 23
train loss: 0.6158919700164427 test loss: 8.018274362222162e+17
epoch 24
train loss: 0.6038621210823999 test loss: 9.139234005551028e+17
epoch 25
train loss: 0.5975748421454897 test loss: 1.0875834525394216e+18
epoch 26
train loss: 0.5952233900647728 test loss: 1.0365881857878675e+18
epoch 27
train loss: 0.5864496195303905 test loss: 1.14415893066999e+18
epoch 28
train loss: 0.5890549263391752 test loss: 1.2599640709384837e+18
epoch 29
train loss: 0.5964843642049039 test loss: 1.1622925351665815e+18
epoch 30
train loss: 0.5854588411299383 test loss: 1.1735900515356037e+18
epoch 31
train loss: 0.5753141442804046 test loss: 1.2825290848539817e+18
epoch 32
train loss: 0.5797910839988687 test loss: 1.709990688336432e+18
epoch 33
train loss: 0.5698670509834444 test loss: 1.531915484011301e+18
epoch 34
train loss: 0.5683211111840176 test loss: 1.5911144211052925e+18
epoch 35
train loss: 0.5676319585566507 test loss: 1.654910065940563e+18
epoch 36
train loss: 0.5691794258704213 test loss: 1.825942029503465e+18
epoch 37
train loss: 0.5667468035651186 test loss: 8.550134695909693e+17
epoch 38
train loss: 0.5581153269115049 test loss: 9.498065499638885e+17
epoch 39
train loss: 0.5526712996590863 test loss: 9.369097390041243e+17
epoch 40
train loss: 0.5513278828779125 test loss: 1.0338998894813928e+18
epoch 41
train loss: 0.5482682806302744 test loss: 1.279367305253434e+18
epoch 42
train loss: 0.5502042275460148 test loss: 1.2758967807004698e+18
epoch 43
train loss: 0.5455953747853751 test loss: 1.3628499723261463e+18
epoch 44
train loss: 0.5445294964339306 test loss: 1.4075456114682637e+18
epoch 45
train loss: 0.5400393261555227 test loss: 1.436217346849773e+18
epoch 46
train loss: 0.5394537987560197 test loss: 1.098045807994043e+18
epoch 47
train loss: 0.5371523495243485 test loss: 1.2448133741832847e+18
epoch 48
train loss: 0.5394652305379487 test loss: 1.3543059193618716e+18
epoch 49
train loss: 0.5322850282019673 test loss: 1.4614187871837504e+18
epoch 50
train loss: 0.5291692181807742 test loss: 1.6818387603938936e+18
epoch 51
train loss: 0.5269139150033885 test loss: 1.5489778442454963e+18
epoch 52
train loss: 0.5287236891345656 test loss: 1.8119789995370534e+18
epoch 53
train loss: 0.5300291420382848 test loss: 1.8563158343665503e+18
epoch 54
train loss: 0.52482717958898 test loss: 1.8408916542896067e+18
epoch 55
train loss: 0.5295224291573105 test loss: 1.7193408788677128e+18
epoch 56
train loss: 0.5226418355338189 test loss: 2.065667722362918e+18
epoch 57
train loss: 0.5217292046004185 test loss: 1.7959801866534781e+18
epoch 58
train loss: 0.5227646026944033 test loss: 1.8057975451995822e+18
epoch 59
train loss: 0.5261356789564031 test loss: 2.5029259338203254e+18
epoch 60
train loss: 0.5254574392595862 test loss: 2.0789772186353815e+18
epoch 61
train loss: 0.521365076069988 test loss: 2.372860064602711e+18
epoch 62
train loss: 0.5158489677118623 test loss: 2.0894236225814656e+18
epoch 63
train loss: 0.5129177284598638 test loss: 2.3609795451711473e+18
epoch 64
train loss: 0.5116773296703562 test loss: 2.059674765685675e+18
epoch 65
train loss: 0.5104017131709815 test loss: 2.16732221003447e+18
epoch 66
train loss: 0.5149076688032962 test loss: 2.4442865481831086e+18
epoch 67
train loss: 0.5114882293302396 test loss: 2.2267991601751619e+18
epoch 68
train loss: 0.5105042667202644 test loss: 2.2079245151264753e+18
epoch 69
train loss: 0.5078867529777092 test loss: 2.1479572406759619e+18
epoch 70
train loss: 0.5086375818280257 test loss: 2.3635460403899474e+18
epoch 71
train loss: 0.5106156401159848 test loss: 2.501428648142226e+18
epoch 72
train loss: 0.507618601113695 test loss: 2.26690599958076e+18
epoch 73
train loss: 0.5071877537394417 test loss: 2.2712889825868756e+18
epoch 74
train loss: 0.5092899859434885 test loss: 2.2710693657114547e+18
epoch 75
train loss: 0.5089004307808961 test loss: 2.1726382557733842e+18
epoch 76
train loss: 0.5081741663045837 test loss: 2.455385318608597e+18
epoch 77
train loss: 0.5028442644201662 test loss: 2.1291432171168369e+18
epoch 78
train loss: 0.5016188407702559 test loss: 2.273319167831653e+18
epoch 79
train loss: 0.5041099974933343 test loss: 2.2014469947494717e+18
epoch 80
train loss: 0.5183376960787834 test loss: 1.9860828004264727e+18
epoch 81
train loss: 0.5321664043275406 test loss: 2.519614880279337e+18
epoch 82
train loss: 0.514455700047035 test loss: 2.4021999015803576e+18
epoch 83
train loss: 0.5008962081311343 test loss: 2.4031023243934413e+18
epoch 84
train loss: 0.4978450417335739 test loss: 2.2611886790162015e+18
epoch 85
train loss: 0.4961809170365647 test loss: 2.395017032295412e+18
epoch 86
train loss: 0.49500444209202715 test loss: 2.3435763222360387e+18
epoch 87
train loss: 0.4941553406657395 test loss: 2.263123692614361e+18
epoch 88
train loss: 0.4942848859695706 test loss: 2.3464728221904026e+18
epoch 89
train loss: 0.49666230527162814 test loss: 2.483898091642457e+18
epoch 90
train loss: 0.4943032685010881 test loss: 2.1012195162816323e+18
epoch 91
train loss: 0.4982596471662417 test loss: 2.4786036945718354e+18
epoch 92
train loss: 0.49558149836426785 test loss: 2.361352719309181e+18
epoch 93
train loss: 0.5074583419237172 test loss: 2.2597459343569021e+18
epoch 94
train loss: 0.502588065479497 test loss: 1.9639477942315556e+18
epoch 95
train loss: 0.49434460776790884 test loss: 1.7674013117517865e+18
epoch 96
train loss: 0.4912472157525815 test loss: 1.85279268638332e+18
epoch 97
train loss: 0.4914355236280129 test loss: 1.9853601477645307e+18
epoch 98
train loss: 0.48932805461529416 test loss: 2.0007621933928965e+18
epoch 99
train loss: 0.4911282374555216 test loss: 2.0468211244841772e+18
epoch 100
train loss: 0.4887757311943929 test loss: 1.9971636228359237e+18
epoch 101
train loss: 0.4887154657143055 test loss: 2.2394490120402465e+18
epoch 102
train loss: 0.4880703037411608 test loss: 2.0608888211262676e+18
epoch 103
train loss: 0.487991565450252 test loss: 2.4614173589799537e+18
epoch 104
train loss: 0.4882196591672682 test loss: 2.091317995453975e+18
epoch 105
train loss: 0.49046310467787174 test loss: 2.234905710446784e+18
epoch 106
train loss: 0.4892853002382127 test loss: 2.371019206853283e+18
epoch 107
train loss: 0.5023027085487894 test loss: 2.83902982994563e+18
epoch 108
train loss: 0.5033019611870357 test loss: 3.97254318089889e+18
epoch 109
train loss: 0.4929734119602934 test loss: 3.19450227345064e+18
epoch 110
train loss: 0.4876206586855449 test loss: 3.1856866936450586e+18
epoch 111
train loss: 0.48636284831412496 test loss: 3.1794415519060255e+18
epoch 112
train loss: 0.48565344519728687 test loss: 2.970518934827328e+18
epoch 113
train loss: 0.48867870458556245 test loss: 2.9710809522560947e+18
epoch 114
train loss: 0.4854740458502859 test loss: 2.718071356851238e+18
epoch 115
train loss: 0.48420594618501644 test loss: 3.0621716640132593e+18
epoch 116
train loss: 0.48672684681616724 test loss: 2.7837239401916457e+18
epoch 117
train loss: 0.48586591271675 test loss: 2.913111201594582e+18
epoch 118
train loss: 0.48418256054576647 test loss: 2.6857203790503153e+18
epoch 119
train loss: 0.4828404785345553 test loss: 2.930831651661307e+18
epoch 120
train loss: 0.4825783173275861 test loss: 2.825677024053145e+18
epoch 121
train loss: 0.48305451921888437 test loss: 2.646867559811063e+18
epoch 122
train loss: 0.487401103847008 test loss: 2.9951399213609303e+18
epoch 123
train loss: 0.491923440267368 test loss: 2.6942084150603515e+18
epoch 124
train loss: 0.48819098150210244 test loss: 2.44320427199648e+18
epoch 125
train loss: 0.49075548369833416 test loss: 2.902841091112562e+18
epoch 126
train loss: 0.48852964271363686 test loss: 3.1016022741053926e+18
epoch 127
train loss: 0.4835534890321311 test loss: 3.376987230554858e+18
epoch 128
train loss: 0.48283848750772196 test loss: 2.502677891750066e+18
epoch 129
train loss: 0.48183868361386767 test loss: 2.8479001874760796e+18
epoch 130
train loss: 0.4817006994898503 test loss: 2.463888373229777e+18
epoch 131
train loss: 0.4794062643675017 test loss: 2.73661989283524e+18
epoch 132
train loss: 0.479523377186461 test loss: 2.473426466513252e+18
epoch 133
train loss: 0.47985750738528904 test loss: 2.6296286084131174e+18
epoch 134
train loss: 0.47900757396607163 test loss: 2.390877660939835e+18
epoch 135
train loss: 0.48005026520121497 test loss: 2.893898278890071e+18
epoch 136
train loss: 0.4793333552162192 test loss: 2.398231395714259e+18
epoch 137
train loss: 0.47903846895374047 test loss: 2.8675156053064054e+18
epoch 138
train loss: 0.4787289653142411 test loss: 2.4655637269812685e+18
epoch 139
train loss: 0.47988425157743797 test loss: 2.7920114886192594e+18
epoch 140
train loss: 0.4796984242465173 test loss: 2.7413149184603433e+18
epoch 141
train loss: 0.48093255871587254 test loss: 2.631616521640978e+18
epoch 142
train loss: 0.48149560764350313 test loss: 2.3133332150406364e+18
epoch 143
train loss: 0.4790460284396846 test loss: 2.9921274297407903e+18
epoch 144
train loss: 0.478253997185533 test loss: 2.3210954789717734e+18
epoch 145
train loss: 0.4782283642614408 test loss: 2.6922160362863754e+18
epoch 146
train loss: 0.47743211498673016 test loss: 2.516575298682436e+18
epoch 147
train loss: 0.4775101678046301 test loss: 2.860973293136723e+18
epoch 148
train loss: 0.4772132400985797 test loss: 2.809612113334094e+18
epoch 149
train loss: 0.48109345602611686 test loss: 3.0359970016765015e+18
epoch 150
train loss: 0.4877473440134758 test loss: 2.9984128226595635e+18
fold 2
epoch 1
> /mnt/c/Users/susan/git/HEA-Toxicity-Prediction/runner.py(161)<module>()
-> wandb.log({"train_loss": train_loss, "test_loss": test_loss, "train_acc": train_acc, "test_acc": test_acc})
*** IndentationError: expected an indented block
*** IndentationError: expected an indented block
*** IndentationError: expected an indented block
> /mnt/c/Users/susan/git/HEA-Toxicity-Prediction/runner.py(162)<module>()
